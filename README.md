## Resume

This project implements machine learning models to perform regression and classification tasks on the Energy Efficiency building dataset. The original "Energy Efficiency" dataset was obtained from the UC Irvine Machine Learning Repository and consists of multivariate data with 768 instances.

The project follows a complete workflow:

1. Exploratory data analysis with correlation matrix

2. Implementation of the Huber cost function and its gradient using vectorization

3. Polynomial transformation for better data fitting

4. Implementation of the Adam algorithm for efficient weight optimization

5. Preprocessing with normalization and train/test split

6. Final models for regression and classification with visualizations

## Especifications

1. Exploratory Analysis: Metadata visualization and correlation matrix

2. Cost Function: Vectorized implementation of Huber loss with adaptive calculation based on the delta parameter

3. Polynomial Transformation: Feature expansion to capture non-linear relationships

4. Adam Optimization: Algorithm that adapts the learning rate using first and second-order moments

5. Preprocessing: Data normalization and stratified train/test split

6. Visualization: Comparative graphs, error histograms, and confusion matrix
    
## Details

The project required multiple dimensional reformatting to adapt the data to the implemented algorithms. Visualization was organized into subplots showing:

- Original data in spreadsheet format with 5 samples

- Cost evolution per iteration during training

- Scatter plots of actual vs. predicted values

- Error distribution histogram with 7 bins

- Confusion matrix for classification model evaluation

For the classification task, continuous data was discretized into 5 categories using a uniform strategy, and the model was trained with logistic regression.

## Results

The project successfully achieved its objectives using machine learning techniques. The confusion matrix shows significant values on the main diagonal, indicating good predictions. The error distribution is concentrated at approximately 2 units, and the residual plot demonstrates adequate dispersion with consistent predictions.

The plot of actual vs. predicted values approximately follows the y = x line, confirming the accuracy of the regression model. The limitations encountered included preprocessing challenges and limited information in the original dataset.

## Visual results

The imagem below is popping out the metadata in terminal:

![WhatsApp Image 2025-09-05 at 20 39 06](https://github.com/user-attachments/assets/2da4d5ec-28b6-41e2-ba07-30a4384ef006)

The result of polinomial regression besides the prediction vs true value in y = x line:

![WhatsApp Image 2025-09-05 at 20 39 07(1)](https://github.com/user-attachments/assets/c299dd98-cbf5-46a7-b936-91dc86cade82)

Quantity of data in each class derived from classification method and distribution error:

![WhatsApp Image 2025-09-05 at 20 39 07](https://github.com/user-attachments/assets/efd6bef0-2e81-4c0b-994d-3c2717b4e7cf)

-- Compilation and libraries

In main file:

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sea

```

At pre training file:

```
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

```

In visualization file:

```
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import KBinsDiscretizer

```

To visualization of the data you should write down:
```
python3 data_visualization.py

```
We use a hierarchical method to manage the files and dependencies. Thus, the data visualization file aggregates all the other files to create a comprehensive visualization of the data generated by them.

## Conclusion

This project demonstrates the successful implementation of complete machine learning pipelines for regression and classification problems, with custom implementation of cost functions, optimization algorithms, and data transformations, resulting in models with good predictive performance.
