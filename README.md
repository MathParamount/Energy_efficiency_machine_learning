## Resume

This project implements machine learning models to perform regression and classification tasks on the Energy Efficiency building dataset. The original "Energy Efficiency" dataset was obtained from the UC Irvine Machine Learning Repository and consists of multivariate data with 768 instances.

The project follows a complete workflow:

1. Exploratory data analysis with correlation matrix

2. Implementation of the Huber cost function and its gradient using vectorization

3. Polynomial transformation for better data fitting

5. Implementation of the Adam algorithm for efficient weight optimization

6. Preprocessing with train/test split and normalization

7. The use of GridSearchCV to chose the best c and adjust of LogisticRegression to penalty: l2

9. Final models for regression and classification with visualizations

## Especifications

1. Exploratory Analysis: Metadata visualization and correlation matrix

2. Cost Function: Vectorized implementation of Huber loss with adaptive calculation based on the delta parameter

3. Polynomial Transformation: Feature expansion to capture non-linear relationships

4. Adding the box-cox in pre_training to adjust the y_train

5. The y_train related to classification part was used the digitize function from numpy to discretize and divide among bins

7. Adam Optimization: Algorithm that adapts the learning rate using first and second-order moments

8. Preprocessing: stratified train/test split and data normalization

9. Visualization: Comparative graphs, error histograms, and confusion matrix
    
## Details

The project required multiple dimensional reformatting to adapt the data to the implemented algorithms. Along the code execution was plotted the number of features that is in 9 features.

The mathematical formule to features verification is:

$$
\binom{n+d}{d}
$$

For the classification task, continuous data was discretized into 5 categories using a uniform strategy, and the model was trained with logistic regression.

The residuos measure follows the mathematical formulation:
$$
residuo = previstion - test_regression
$$

## Results

The project successfully achieved its objectives using machine learning techniques. The confusion matrix shows significant values on the main diagonal, indicating good predictions. The error distribution is concentrated at zero units, and the residual plot demonstrates adequate dispersion with consistent predictions.

The plot of actual vs. predicted values approximately follows the y = x line, confirming the accuracy of the regression model, even been in 67% of accuracy. The limitations encountered included preprocessing challenges and limited information in the original dataset.

## Visual results

The result of polinomial regression besides the prediction vs true value in y = x line:

![regression](https://github.com/user-attachments/assets/7edbee15-ca72-4d4c-b5a2-4b698c82c7c0)

Original data and with box-cox:

![original_vs_boxcox](https://github.com/user-attachments/assets/12864af2-e416-4d5d-b4eb-693d89b5f2dc)

The residual plot:

![residuo](https://github.com/user-attachments/assets/b0db6c82-8392-4326-8268-b02876be2b49)


## Compilation and libraries

In main file:

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sea

```

At pre training file:

```
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

```

In visualization file:

```
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import KBinsDiscretizer

```

To visualization of the data you should write down:
```
python3 data_visualization.py

```
We use a hierarchical method to manage the files and dependencies. Thus, the data visualization file aggregates all the other files to create a comprehensive visualization of the data generated by them.

## Conclusion

This project demonstrates the successful implementation of complete machine learning pipelines for regression and classification problems, with custom implementation of cost functions, optimization algorithms, and data transformations, resulting in models with good predictive performance.
